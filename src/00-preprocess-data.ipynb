{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from data_prep_helper import simple_imputer, getSentences\n",
    "import nltk\n",
    "import re\n",
    "import warnings\n",
    "import spacy\n",
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Helper functions\n",
    "def simple_imputer(df):\n",
    "    idx = pd.IndexSlice\n",
    "    df = df.copy()\n",
    "    ID_COLS = ['subject_id', 'hadm_id', 'icustay_id']\n",
    "    if len(df.columns.names) > 2: df.columns = df.columns.droplevel(('label', 'LEVEL1', 'LEVEL2'))\n",
    "\n",
    "    df_out = df.loc[:, idx[:, ['mean', 'count']]]\n",
    "    icustay_means = df_out.loc[:, idx[:, 'mean']].groupby(ID_COLS).mean()\n",
    "\n",
    "    df_out.loc[:, idx[:, 'mean']] = df_out.loc[:, idx[:, 'mean']].groupby(ID_COLS).fillna(\n",
    "        method='ffill'\n",
    "    ).groupby(ID_COLS).fillna(icustay_means).fillna(0)\n",
    "\n",
    "    df_out.loc[:, idx[:, 'count']] = (df.loc[:, idx[:, 'count']] > 0).astype(float)\n",
    "    df_out.rename(columns={'count': 'mask'}, level='Aggregation Function', inplace=True)\n",
    "\n",
    "    is_absent = (1 - df_out.loc[:, idx[:, 'mask']])\n",
    "    hours_of_absence = is_absent.cumsum()\n",
    "    time_since_measured = hours_of_absence - hours_of_absence[is_absent == 0].fillna(method='ffill')\n",
    "    time_since_measured.rename(columns={'mask': 'time_since_measured'}, level='Aggregation Function', inplace=True)\n",
    "\n",
    "    df_out = pd.concat((df_out, time_since_measured), axis=1)\n",
    "    df_out.loc[:, idx[:, 'time_since_measured']] = df_out.loc[:, idx[:, 'time_since_measured']].fillna(100)\n",
    "\n",
    "    df_out.sort_index(axis=1, inplace=True)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "SECTION_TITLES = re.compile(\n",
    "    r'('\n",
    "    r'ABDOMEN AND PELVIS|CLINICAL HISTORY|CLINICAL INDICATION|COMPARISON|COMPARISON STUDY DATE'\n",
    "    r'|EXAM|EXAMINATION|FINDINGS|HISTORY|IMPRESSION|INDICATION'\n",
    "    r'|MEDICAL CONDITION|PROCEDURE|REASON FOR EXAM|REASON FOR STUDY|REASON FOR THIS EXAMINATION'\n",
    "    r'|TECHNIQUE'\n",
    "    r'):|FINAL REPORT',\n",
    "    re.I | re.M)\n",
    "\n",
    "\n",
    "def getSentences(t):\n",
    "    return list(preprocess_mimic(t))\n",
    "\n",
    "\n",
    "def pattern_repl(matchobj):\n",
    "    \"\"\"\n",
    "    Return a replacement string to be used for match object\n",
    "    \"\"\"\n",
    "    return ' '.rjust(len(matchobj.group(0)))\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace [**Patterns**] with spaces.\n",
    "    text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', pattern_repl, text)\n",
    "    # Replace `_` with spaces.\n",
    "    text = re.sub(r'_', ' ', text)\n",
    "\n",
    "    start = 0\n",
    "    end = find_end(text)\n",
    "    new_text = ''\n",
    "    if start > 0:\n",
    "        new_text += ' ' * start\n",
    "    new_text = text[start:end]\n",
    "\n",
    "    # make sure the new text has the same length of old text.\n",
    "    if len(text) - end > 0:\n",
    "        new_text += ' ' * (len(text) - end)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def preprocess_mimic(text):\n",
    "    \"\"\"\n",
    "    Preprocess reports in MIMIC-III.\n",
    "    1. remove [**Patterns**] and signature\n",
    "    2. split the report into sections\n",
    "    3. tokenize sentences and words\n",
    "    4. lowercase\n",
    "    \"\"\"\n",
    "    for sec in split_heading(clean_text(text)):\n",
    "        for sent in sent_tokenize(sec):\n",
    "            text = ' '.join(word_tokenize(sent))\n",
    "            yield text.lower()\n",
    "\n",
    "\n",
    "def split_heading(text):\n",
    "    \"\"\"Split the report into sections\"\"\"\n",
    "    start = 0\n",
    "    for matcher in SECTION_TITLES.finditer(text):\n",
    "        # add last\n",
    "        end = matcher.start()\n",
    "        if end != start:\n",
    "            section = text[start:end].strip()\n",
    "            if section:\n",
    "                yield section\n",
    "\n",
    "        # add title\n",
    "        start = end\n",
    "        end = matcher.end()\n",
    "        if end != start:\n",
    "            section = text[start:end].strip()\n",
    "            if section:\n",
    "                yield section\n",
    "\n",
    "        start = end\n",
    "\n",
    "    # add last piece\n",
    "    end = len(text)\n",
    "    if start < end:\n",
    "        section = text[start:end].strip()\n",
    "        if section:\n",
    "            yield section\n",
    "\n",
    "\n",
    "def find_end(text):\n",
    "    \"\"\"Find the end of the report.\"\"\"\n",
    "    ends = [len(text)]\n",
    "    patterns = [\n",
    "        re.compile(r'BY ELECTRONICALLY SIGNING THIS REPORT', re.I),\n",
    "        re.compile(r'\\n {3,}DR.', re.I),\n",
    "        re.compile(r'[ ]{1,}RADLINE ', re.I),\n",
    "        re.compile(r'.*electronically signed on', re.I),\n",
    "        re.compile(r'M\\[0KM\\[0KM')\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        matchobj = pattern.search(text)\n",
    "        if matchobj:\n",
    "            ends.append(matchobj.start())\n",
    "    return min(ends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIMESERIES DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMIC_EXTRACT_DATA = \"../data/all_hourly_data.h5\"\n",
    "ts_data_lvl2 = pd.read_hdf(MIMIC_EXTRACT_DATA, \"vitals_labs\")\n",
    "ts_data_raw= pd.read_hdf(MIMIC_EXTRACT_DATA, \"vitals_labs\")\n",
    "static_data = pd.read_hdf(MIMIC_EXTRACT_DATA, 'patients')\n",
    "SEED = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Inclusion-Exclusion Criteria applied for time series data\n",
    "##At least 24+6(gap) hours in ICU\n",
    "GAP_TIME = 6  # In hours\n",
    "WINDOW_SIZE = 24  # In hours\n",
    "Ys = static_data[static_data.max_hours > WINDOW_SIZE + GAP_TIME][['mort_hosp', 'mort_icu', 'los_icu']]\n",
    "Ys['los_3'] = Ys['los_icu'] > 3\n",
    "Ys['los_7'] = Ys['los_icu'] > 7\n",
    "Ys.drop(columns=['los_icu'], inplace=True)\n",
    "Ys.astype(float)\n",
    "\n",
    "lvl2, raw = [df[\n",
    "                 (df.index.get_level_values('icustay_id').isin(set(Ys.index.get_level_values('icustay_id')))) &\n",
    "                 (df.index.get_level_values('hours_in') < WINDOW_SIZE)\n",
    "                 ] for df in (ts_data_lvl2, ts_data_raw)]\n",
    "raw.columns = raw.columns.droplevel(level=['LEVEL2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split time series data into train/dev/test\n",
    "train_frac, dev_frac, test_frac = 0.7, 0.1, 0.2\n",
    "lvl2_subj_idx, raw_subj_idx, Ys_subj_idx = [df.index.get_level_values('subject_id') for df in (lvl2, raw, Ys)]\n",
    "lvl2_subjects = set(lvl2_subj_idx)\n",
    "assert lvl2_subjects == set(Ys_subj_idx), \"Subject ID pools differ!\"\n",
    "assert lvl2_subjects == set(raw_subj_idx), \"Subject ID pools differ!\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "subjects, N = np.random.permutation(list(lvl2_subjects)), len(lvl2_subjects)\n",
    "N_train, N_dev, N_test = int(train_frac * N), int(dev_frac * N), int(test_frac * N)\n",
    "train_subj = subjects[:N_train]\n",
    "dev_subj = subjects[N_train:N_train + N_dev]\n",
    "test_subj = subjects[N_train + N_dev:]\n",
    "\n",
    "[(lvl2_train, lvl2_dev, lvl2_test), (raw_train, raw_dev, raw_test), (Ys_train, Ys_dev, Ys_test)] = [\n",
    "    [df[df.index.get_level_values('subject_id').isin(s)] for s in (train_subj, dev_subj, test_subj)] \\\n",
    "    for df in (lvl2, raw, Ys)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Normalize time series data\n",
    "idx = pd.IndexSlice\n",
    "lvl2_means, lvl2_stds = lvl2_train.loc[:, idx[:, 'mean']].mean(axis=0), lvl2_train.loc[:, idx[:, 'mean']].std(axis=0)\n",
    "\n",
    "lvl2_train.loc[:, idx[:, 'mean']] = (lvl2_train.loc[:, idx[:, 'mean']] - lvl2_means) / lvl2_stds\n",
    "lvl2_dev.loc[:, idx[:, 'mean']] = (lvl2_dev.loc[:, idx[:, 'mean']] - lvl2_means) / lvl2_stds\n",
    "lvl2_test.loc[:, idx[:, 'mean']] = (lvl2_test.loc[:, idx[:, 'mean']] - lvl2_means) / lvl2_stds\n",
    "\n",
    "lvl2_train, lvl2_dev, lvl2_test = [\n",
    "    simple_imputer(df) for df in (lvl2_train, lvl2_dev, lvl2_test)\n",
    "]\n",
    "lvl2_flat_train, lvl2_flat_dev, lvl2_flat_test = [\n",
    "    df.pivot_table(index=['subject_id', 'hadm_id', 'icustay_id'], columns=['hours_in']) for df in (\n",
    "        lvl2_train, lvl2_dev, lvl2_test\n",
    "    )\n",
    "]\n",
    "\n",
    "for df in lvl2_train, lvl2_dev, lvl2_test: assert not df.isnull().any().any()\n",
    "\n",
    "[(Ys_train, Ys_dev, Ys_test)] = [\n",
    "    [df[df.index.get_level_values('subject_id').isin(s)] for s in (train_subj, dev_subj, test_subj)] \\\n",
    "    for df in (Ys,)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of train, dev, test {}, {}, {}.\".format(lvl2_train.shape, lvl2_dev.shape, lvl2_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save datasets in pickle file\n",
    "pd.to_pickle(lvl2_train, \"../data/timeseries/lvl2_imputer_train.pkl\")\n",
    "pd.to_pickle(lvl2_dev, \"../data/timeseries/lvl2_imputer_dev.pkl\")\n",
    "pd.to_pickle(lvl2_test, \"../data/timeseries/lvl2_imputer_test.pkl\")\n",
    "\n",
    "pd.to_pickle(Ys, \"../data/timeseries/Ys.pkl\")\n",
    "pd.to_pickle(Ys_train, \"../data/timeseries/Ys_train.pkl\")\n",
    "pd.to_pickle(Ys_dev, \"../data/timeseries/Ys_dev.pkl\")\n",
    "pd.to_pickle(Ys_test, \"../data/timeseries/Ys_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical Notes Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admission_df = pd.read_csv(\"../data/ADMISSIONS.csv\")\n",
    "noteevents_df = pd.read_csv(\"../data/NOTEEVENTS.csv\")\n",
    "icustays_df = pd.read_csv(\"../data/ICUSTAYS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = []  # store all patient ids\n",
    "for each_entry in Ys.index:\n",
    "    patient_ids.append(each_entry[0])\n",
    "note_categories = noteevents_df.groupby(noteevents_df.CATEGORY).agg(['count']).index\n",
    "selected_note_types = []\n",
    "for each_cat in list(note_categories):\n",
    "    if each_cat != 'Discharge summary':\n",
    "        selected_note_types.append(each_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop discharge summaries to avoid information leak\n",
    "sub_notes = noteevents_df[noteevents_df.CATEGORY.isin(selected_note_types)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop clinical notes with no chart times\n",
    "missing_chardate_index = []\n",
    "for each_note in sub_notes.itertuples():\n",
    "    if isinstance(each_note.CHARTTIME, str):\n",
    "        continue\n",
    "    if np.isnan(each_note.CHARTTIME):\n",
    "        missing_chardate_index.append(each_note.Index)\n",
    "print(\"{} of notes does not have charttime.\".format(len(missing_chardate_index)))\n",
    "\n",
    "sub_notes.drop(missing_chardate_index, inplace=True)\n",
    "print(\"After dropping no notes, the note shape is {}\".format(sub_notes.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Drop patients with no clinical notes in 24 hours\n",
    "sub_notes = sub_notes[sub_notes.SUBJECT_ID.isin(patient_ids)]\n",
    "TIMELIMIT = 1  # 1day\n",
    "new_static = static_data.reset_index()\n",
    "new_static.rename(columns={\"subject_id\": \"SUBJECT_ID\", \"hadm_id\": \"HADM_ID\"}, inplace=True)\n",
    "print(\"New Stats shape is {}\".format(new_static.shape))\n",
    "print(\"Sub note shape is {}\".format(sub_notes.shape))\n",
    "df_adm_notes = pd.merge(sub_notes[['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTTIME', 'CATEGORY', 'TEXT']],\n",
    "                        new_static[['SUBJECT_ID', 'HADM_ID', 'icustay_id', 'age', 'admittime', 'dischtime', 'deathtime',\n",
    "                                   'intime', 'outtime', 'los_icu', 'mort_icu', 'mort_hosp', 'hospital_expire_flag',\n",
    "                                   'hospstay_seq', 'max_hours']],\n",
    "                        on=['SUBJECT_ID'],\n",
    "                        how='left')\n",
    "\n",
    "df_adm_notes['CHARTTIME'] = pd.to_datetime(df_adm_notes['CHARTTIME'])\n",
    "df_less_n = df_adm_notes[\n",
    "    ((df_adm_notes['CHARTTIME'] - df_adm_notes['intime']).dt.total_seconds() / (24 * 60 * 60)) < TIMELIMIT]\n",
    "pd.to_pickle(df_less_n, \"../data/clinical_notes/sub_notes.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process clinical notes\n",
    "sub_notes = df_less_n[df_less_n.SUBJECT_ID.notnull()]\n",
    "sub_notes = sub_notes[sub_notes.CHARTTIME.notnull()]\n",
    "sub_notes = sub_notes[sub_notes.TEXT.notnull()]\n",
    "sub_notes = sub_notes[['SUBJECT_ID', 'HADM_ID_y', 'CHARTTIME', 'TEXT']]\n",
    "sub_notes['preprocessed_text'] = None\n",
    "for each_note in sub_notes.itertuples():\n",
    "    text = each_note.TEXT\n",
    "    sub_notes.at[each_note.Index, 'preprocessed_text'] = getSentences(text)\n",
    "pd.to_pickle(sub_notes, \"../data/clinical_notes/preprocessed_notes.p\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse6250-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
